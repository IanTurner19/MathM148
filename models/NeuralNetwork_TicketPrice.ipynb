{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final.csv\")\n",
    "df[\"EVENT_START_DATETIME\"] = pd.to_datetime(df[\"EVENT_START_DATETIME\"])\n",
    "event_start = pd.to_datetime(df[\"EVENT_START_DATETIME\"]).astype(np.int64) // 10**9\n",
    "scrape_time = pd.to_datetime(df[\"scrape_time\"]).astype(np.int64) // 10**9\n",
    "df[\"time_to_concert\"] =  event_start - scrape_time\n",
    "\n",
    "localized_times = df.apply(lambda row: row['EVENT_START_DATETIME'].tz_convert(row['VENUE_TIMEZONE']), axis=1)\n",
    "\n",
    "def get_day_of_week(dt):\n",
    "    return dt.weekday()\n",
    "df[\"day_of_week\"] = localized_times.map(get_day_of_week)\n",
    "\n",
    "def get_hour(dt):\n",
    "    return dt.hour\n",
    "df[\"hour_of_day\"] = localized_times.map(get_hour)\n",
    "to_keep = [\n",
    "    \"CLASSIFICATION_GENRE\",\n",
    "    \"CLASSIFICATION_SUB_GENRE\",\n",
    "    \"MIN_PRICE\", \n",
    "    \"MAX_PRICE\",\n",
    "    \"HOT_EVENT\",\n",
    "    \"CAPACITY\",\n",
    "    \"population\",\n",
    "    \"time_to_concert\",\n",
    "    \"artist_popularity\",\n",
    "    \"TRANSACTABLE\",\n",
    "    \"day_of_week\",\n",
    "    \"hour_of_day\"\n",
    "]\n",
    "df = df[to_keep]\n",
    "df[\"CLASSIFICATION_SUB_GENRE\"] = df['CLASSIFICATION_SUB_GENRE'].fillna(df['CLASSIFICATION_GENRE'])\n",
    "df.dropna(inplace=True)\n",
    "categorical_cols = [col_name for col_name in df.columns if df[col_name].dtype == \"object\"]\n",
    "X = pd.get_dummies(df.drop([\"MIN_PRICE\", \"MAX_PRICE\"], axis=1), columns=categorical_cols).to_numpy()\n",
    "min_y = df['MIN_PRICE'].to_numpy()\n",
    "max_y = df[\"MAX_PRICE\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_min_train, y_min_test = train_test_split(X, min_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available(): # my current system is Mac Silicon so I will be using the GPU associated with that\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # revert the device back to the cpu if mps is not avilable\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    print(\"Falling back to CPU device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "y_min_train = np.array(y_min_train, dtype = np.float32)\n",
    "y_min_test = np.array(y_min_test, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train).squeeze() \n",
    "y_min_train_tensor = torch.from_numpy(y_min_train).squeeze()\n",
    "X_test_tensor = torch.from_numpy(X_test).squeeze()\n",
    "y_min_test_tensor = torch.from_numpy(y_min_test).squeeze() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class used to format data into Dataset that is compatible with data loader\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Dataset: abstract class that provides structure for PyTorch Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Initializes the CustomDataset with X data and y data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: torch tensor\n",
    "            Feature Data\n",
    "        y : torch tensor\n",
    "            Target Variable\n",
    "        \"\"\"\n",
    "        self.X  = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Obtains the length of the CustomDataset\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Allows  the CustomDataset to be indexed\n",
    "        \"\"\"\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(X_train_tensor, y_min_train_tensor)\n",
    "test_data = CustomDataset(X_test_tensor, y_min_test_tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_data,\n",
    "                           batch_size = 10, shuffle = True, num_workers = 0)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fully_connected_nn(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to define our fully connected Neural Network\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    nn.Module: parent class that contains methods for creating neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with all necessary layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(52, 64) # inputs the 33 features and feeds it through a layer with width = 64\n",
    "        self.layer2 = nn.Linear(64,64) # layer with width 66\n",
    "        self.layer3 = nn.Linear(64, 1) # output a single value for prediction\n",
    "\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        defines the process when data is passed through the neural network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch tensor\n",
    "            Defines the data to pass through the tensor\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.view(-1, 52) # make sure the data is correctly formatted, 52 features\n",
    "\n",
    "        out = self.layer1(x) # first layer \n",
    "        out = torch.relu(out) # ReLU function for non linearity\n",
    "\n",
    "        out = self.layer2(out) # second layer\n",
    "        out = torch.relu(out) # ReLU function for non linearity\n",
    "\n",
    "        out = self.layer3(out) # third layer\n",
    "           \n",
    "\n",
    "        return out # return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = fully_connected_nn()\n",
    "model_nn = model_nn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_nn.parameters(), lr = 0.1)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min NN Predcition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Time: 1.37 sec\n",
      "Train Loss: 401817074530.6902, Test Loss: 2207.7432\n",
      "Epoch [10/100], Time: 12.4 sec\n",
      "Train Loss: 1327.9517, Test Loss: 1208.7710\n",
      "Epoch [20/100], Time: 23.75 sec\n",
      "Train Loss: 1326.2743, Test Loss: 1206.1139\n",
      "Epoch [30/100], Time: 35.42 sec\n",
      "Train Loss: 1319.6931, Test Loss: 1197.6754\n",
      "Epoch [40/100], Time: 46.72 sec\n",
      "Train Loss: 1306.6580, Test Loss: 1182.3901\n",
      "Epoch [50/100], Time: 58.05 sec\n",
      "Train Loss: 1296.9039, Test Loss: 1163.1656\n",
      "Epoch [60/100], Time: 69.29 sec\n",
      "Train Loss: 1223.9571, Test Loss: 1083.9795\n",
      "Epoch [70/100], Time: 80.5 sec\n",
      "Train Loss: 1077.0696, Test Loss: 942.6922\n",
      "Epoch [80/100], Time: 91.89 sec\n",
      "Train Loss: 857.0898, Test Loss: 720.2616\n",
      "Epoch [90/100], Time: 103.04 sec\n",
      "Train Loss: 584.5089, Test Loss: 446.8935\n",
      "Epoch [100/100], Time: 114.52 sec\n",
      "Train Loss: 346.0352, Test Loss: 242.6574\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()  # Start timer\n",
    "num_epochs = 101  # Initialize the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):  # Iterate through epochs\n",
    "    running_loss = 0  # Initialize training loss\n",
    "    running_loss_test = 0  # Initialize testing loss\n",
    "\n",
    "    model_nn.train()  # Put model into training mode\n",
    "\n",
    "    for batch_data, batch_labels in train_loader:  # Get data batches\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)  # Move to GPU if needed\n",
    "        outputs = model_nn(batch_data).squeeze()  # Forward pass\n",
    "        loss = criterion(outputs, batch_labels)  # Compute MSE loss\n",
    "\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()  # Update training loss\n",
    "\n",
    "\n",
    "    # Evaluation phase\n",
    "    model_nn.eval()  # Put model into evaluation mode\n",
    "    all_preds_test = []  # Store test predictions\n",
    "    all_labels_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data_test, batch_labels_test in test_loader:\n",
    "            batch_data_test, batch_labels_test = batch_data_test.to(device), batch_labels_test.to(device)\n",
    "            test_outputs = model_nn(batch_data_test).squeeze()  # Forward pass\n",
    "            test_loss = criterion(test_outputs, batch_labels_test.float().squeeze())  # Compute test loss\n",
    "\n",
    "            running_loss_test += test_loss.item()  # Update test loss\n",
    "            all_preds_test.extend(test_outputs.cpu().numpy().flatten())\n",
    "            all_labels_test.extend(batch_labels_test.cpu().numpy().flatten())\n",
    "\n",
    "\n",
    "    if (epoch) % 10 == 0:  # Print every 10 epochs\n",
    "        print(f\"Epoch [{epoch}/{num_epochs-1}], Time: {round(time.time() - start, 2)} sec\")\n",
    "        print(f\"Train Loss: {running_loss/len(train_loader):.4f}, Test Loss: {running_loss_test/len(test_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete.\")  # Confirm training completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max NN Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_max_train, y_max_test = train_test_split(X, max_y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "y_max_train = np.array(y_max_train, dtype = np.float32)\n",
    "y_max_test = np.array(y_max_test, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train).squeeze() \n",
    "y_max_train_tensor = torch.from_numpy(y_max_train).squeeze()\n",
    "X_test_tensor = torch.from_numpy(X_test).squeeze()\n",
    "y_max_test_tensor = torch.from_numpy(y_max_test).squeeze() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(X_train_tensor, y_max_train_tensor)\n",
    "test_data = CustomDataset(X_test_tensor, y_max_test_tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_data,\n",
    "                           batch_size = 10, shuffle = True, num_workers = 0)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Time: 1.37 sec\n",
      "Train Loss: 16438.5735, Test Loss: 9065.1093\n",
      "Epoch [10/100], Time: 13.64 sec\n",
      "Train Loss: 15016.8084, Test Loss: 8134.9933\n",
      "Epoch [20/100], Time: 25.63 sec\n",
      "Train Loss: 14911.1695, Test Loss: 8158.5663\n",
      "Epoch [30/100], Time: 37.83 sec\n",
      "Train Loss: 14916.2252, Test Loss: 8172.2402\n",
      "Epoch [40/100], Time: 51.36 sec\n",
      "Train Loss: 14912.8714, Test Loss: 8171.1292\n",
      "Epoch [50/100], Time: 64.87 sec\n",
      "Train Loss: 14914.7381, Test Loss: 8173.8925\n",
      "Epoch [60/100], Time: 77.23 sec\n",
      "Train Loss: 14910.7257, Test Loss: 8170.4131\n",
      "Epoch [70/100], Time: 89.4 sec\n",
      "Train Loss: 14917.6117, Test Loss: 8171.8787\n",
      "Epoch [80/100], Time: 103.78 sec\n",
      "Train Loss: 14907.5453, Test Loss: 8172.4904\n",
      "Epoch [90/100], Time: 116.25 sec\n",
      "Train Loss: 14912.0808, Test Loss: 8171.2051\n",
      "Epoch [100/100], Time: 128.75 sec\n",
      "Train Loss: 14908.9411, Test Loss: 8169.8372\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()  # Start timer\n",
    "num_epochs = 101  # Initialize the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):  # Iterate through epochs\n",
    "    running_loss = 0  # Initialize training loss\n",
    "    running_loss_test = 0  # Initialize testing loss\n",
    "\n",
    "    model_nn.train()  # Put model into training mode\n",
    "\n",
    "    for batch_data, batch_labels in train_loader:  # Get data batches\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)  # Move to GPU if needed\n",
    "        outputs = model_nn(batch_data).squeeze()  # Forward pass\n",
    "        loss = criterion(outputs, batch_labels)  # Compute MSE loss\n",
    "\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()  # Update training loss\n",
    "\n",
    "\n",
    "    # Evaluation phase\n",
    "    model_nn.eval()  # Put model into evaluation mode\n",
    "    all_preds_test = []  # Store test predictions\n",
    "    all_labels_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data_test, batch_labels_test in test_loader:\n",
    "            batch_data_test, batch_labels_test = batch_data_test.to(device), batch_labels_test.to(device)\n",
    "            test_outputs = model_nn(batch_data_test).squeeze()  # Forward pass\n",
    "            test_loss = criterion(test_outputs, batch_labels_test.float().squeeze())  # Compute test loss\n",
    "\n",
    "            running_loss_test += test_loss.item()  # Update test loss\n",
    "            all_preds_test.extend(test_outputs.cpu().numpy().flatten())\n",
    "            all_labels_test.extend(batch_labels_test.cpu().numpy().flatten())\n",
    "\n",
    "\n",
    "    if (epoch) % 10 == 0:  # Print every 10 epochs\n",
    "        print(f\"Epoch [{epoch}/{num_epochs-1}], Time: {round(time.time() - start, 2)} sec\")\n",
    "        print(f\"Train Loss: {running_loss/len(train_loader):.4f}, Test Loss: {running_loss_test/len(test_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete.\")  # Confirm training completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
